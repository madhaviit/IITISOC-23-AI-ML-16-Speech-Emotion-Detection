{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "FM9g3V1xHSyd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj26qdL6Gu6f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "import soundfile\n",
        "import os, glob, pickle\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "from keras.utils import np_utils, to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Dataset"
      ],
      "metadata": {
        "id": "BvMWPKtaHZYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the Dataset\n",
        "features = pd.read_csv(\"/content/features (2).csv\") #Path of the csv file\n",
        "X = features.iloc[:,:-1].values\n",
        "Y = features.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "WxykK4QYHdkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding the dependent variable\n",
        "encoder = OneHotEncoder()\n",
        "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
      ],
      "metadata": {
        "id": "8ETEGGwiHtnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "dFJlXTv4H2gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling the data with sklearn's Standard scaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "G2Xw4hHhH_IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape[1]"
      ],
      "metadata": {
        "id": "lb-QXDIsQ84u",
        "outputId": "149c5247-95e7-495f-ef9b-f37c942b54d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "182"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(units=6, activation='softmax'))\n",
        "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "e0MVElcGIJl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n",
        "history=model.fit(x_train, y_train, batch_size=256, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])"
      ],
      "metadata": {
        "id": "qqeksneoIOdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n",
        "\n",
        "epochs = [i for i in range(1)]\n",
        "fig , ax = plt.subplots(1,2)\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "test_acc = history.history['val_accuracy']\n",
        "test_loss = history.history['val_loss']\n",
        "\n",
        "fig.set_size_inches(20,6)\n",
        "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
        "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
        "ax[0].set_title('Training & Testing Loss')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "\n",
        "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
        "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
        "ax[1].set_title('Training & Testing Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ht65RcefIQve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "b3bL2726KlKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Pkl_Filename = \"Emotion_Voice_Detection_Model.pkl\"\n",
        "with open(Pkl_Filename, 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ],
      "metadata": {
        "id": "9ICR5C0rKkLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}